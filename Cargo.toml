[package]
name = "llm-terminal-chat"
version = "0.1.0"
edition = "2021"

[dependencies]
# Use local Candle with fixes
candle-core = { path = "./candle-local/candle-core", features = ["cuda"] }
candle-nn = { path = "./candle-local/candle-nn" }
candle-transformers = { path = "./candle-local/candle-transformers" }

# Tokenizer support
tokenizers = { version = "0.20", features = ["http"] }
hf-hub = { version = "0.3", features = ["tokio"] }

# Terminal UI
cursive = { version = "0.21", default-features = false, features = ["crossterm-backend"] }

# Async runtime
tokio = { version = "1.40", features = ["full"] }

# Error handling and utilities
anyhow = "1.0"
clap = { version = "4.5", features = ["derive"] }
tracing = "0.1"
tracing-subscriber = "0.3"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Make sure we use consistent rand version
rand = "0.8.5"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
